# Project Configuration

# Data Paths
data:
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  dataset_path: "data/processed/formality_dataset.jsonl"
  corpora:
    aozora: "data/raw/aozora"
    wikipedia: "data/raw/wikipedia"
    ted: "data/raw/ted"
    kftt: "data/raw/kftt"
    jddc: "data/raw/jddc"
    tatoeba: "data/raw/tatoeba"

# Formality Labeling
labeling:
  casual_markers: ["だ", "か", "よ", "ない"] # Simplified list, logic in code
  polite_markers: ["です", "ます", "でしょうか"]
  humble_markers: ["いたします", "いただきます", "参る"]
  honorific_markers: ["召し上がる", "なさる", "いらっしゃる"]

# Model Configuration
models:
  embedding_model: "cl-tohoku/bert-base-japanese-v3" # or "sentence-transformers/LaBSE"
  regressor_save_path: "models/formality_regressor.pt"
  batch_size: 32
  learning_rate: 1.0e-4
  epochs: 10
  dropout: 0.1
  hidden_dim: 256

# LLM API Configuration
llm:
  openai_model: "gpt-4-0613"
  anthropic_model: "claude-2"
  local_model_path: "models/llama-2-7b-chat.gguf"
  temperature: 0.7
  max_tokens: 150
  retry_attempts: 3
  backoff_factor: 2

# Analysis
analysis:
  output_dir: "results"
  figures_dir: "results/figures"
